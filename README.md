# LLM Lab

<div align="center">
  <img src="assets/logo/llm-lab-logo.png" width="200" alt="LLM Lab Logo">
  <h3>ğŸš€ ä¸€ä¸ªç”¨äºæµ‹è¯• OpenAI-compatible æ¨¡å‹çš„å•å…¥å£å·¥å…·</h3>
  <p>æ”¯æŒ CLI + WebUI å¤šæ¨¡æ€å¯¹è¯ | å¤š Profile ç®¡ç† | ä¼šè¯è¿½è¸ª</p>

  <div style="display: flex; gap: 8px; justify-content: center; flex-wrap: wrap;">
    <img src="https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python" alt="Python Version">
    <img src="https://img.shields.io/badge/OpenAI-API-green?style=flat-square&logo=openai" alt="OpenAI API">
    <img src="https://img.shields.io/badge/Streamlit-WebUI-red?style=flat-square&logo=streamlit" alt="Streamlit">
    <img src="https://img.shields.io/badge/Gradio-WebUI-orange?style=flat-square&logo=gradio" alt="Gradio">
    <img src="https://img.shields.io/badge/FastAPI-WebUI-009688?style=flat-square&logo=fastapi" alt="FastAPI">
    <img src="https://img.shields.io/badge/Multimodal-Support-purple?style=flat-square&logo=ai" alt="Multimodal">
    <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square&logo=opensourceinitiative" alt="License">
  </div>

  <p style="margin-top: 16px;">
    <a href="README_EN.md">English Version</a> | ä¸­æ–‡ç‰ˆæœ¬
  </p>
</div>

---

## ğŸ¯ é¡¹ç›®äº®ç‚¹

### æ ¸å¿ƒä¼˜åŠ¿
- **å•å…¥å£å‘½ä»¤**: åªéœ€ `llm-lab` ä¸€ä¸ªå‘½ä»¤ï¼Œç»Ÿä¸€æ‰€æœ‰äº¤äº’
- **å¤šç§äº¤äº’æ–¹å¼**:
  - **CLI**: åŸºäº Rich çš„ç¾è§‚ç»ˆç«¯ç•Œé¢ï¼Œæ”¯æŒæµå¼è¾“å‡º
  - **WebUI**: æä¾› Streamlitã€Gradioã€FastAPI(HTML) ä¸‰ç§ç°ä»£åŒ–ç•Œé¢
- **æ™ºèƒ½é…ç½®ç®¡ç†**: å¤š Profile ç®¡ç†ï¼Œå¿«é€Ÿåˆ‡æ¢ä¸åŒæ¨¡å‹å’ŒæœåŠ¡å•†
- **å¤šæ¨¡æ€æ”¯æŒ**:
  - ğŸ–¼ï¸ å›¾ç‰‡ç›´ä¼ 
  - ğŸ¥ è§†é¢‘è‡ªåŠ¨æŠ½å¸§
- **ä¼šè¯è¿½è¸ª**: å®Œæ•´çš„èŠå¤©è®°å½•ä¿å­˜ã€åŠ è½½ã€å›æ”¾åŠŸèƒ½

---

## ğŸ”„ è¿è¡Œæµç¨‹

```mermaid
flowchart TD
    A[llm-lab å¯åŠ¨] --> B{æ¨¡å¼é€‰æ‹©}
    B -->|chat| C[CLI ç»ˆç«¯]
    B -->|web| D[WebUI]
    D -->|streamlit| E[Streamlit ç•Œé¢]
    D -->|gradio| F[Gradio ç•Œé¢]
    D -->|fastapi| O[FastAPI + HTML ç•Œé¢]
    C --> G[åŠ è½½ .env ä¸ profiles.yaml]
    E --> G
    F --> G
    O --> G
    G --> H[æ„å»º OpenAI å…¼å®¹å®¢æˆ·ç«¯]
    H --> I{è¾“å…¥ç±»å‹}
    I -->|æ–‡æœ¬| J[Chat Completion]
    I -->|å›¾ç‰‡| K[Vision Prompt]
    I -->|è§†é¢‘| L[è‡ªåŠ¨æŠ½å¸§]
    L --> K
    J --> M[è¾“å‡ºå›å¤ä¸ Token ç»Ÿè®¡]
    K --> M
    M --> N[å¯é€‰ï¼šä¼šè¯è½ç›˜ storage/conversations]
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```text
.
â”œâ”€â”€ app/                  # å…¥å£å±‚ï¼ˆmain / cli / webï¼‰
â”œâ”€â”€ service/              # æ ¸å¿ƒæœåŠ¡ï¼ˆchat / capability / sessionï¼‰
â”œâ”€â”€ utils/                # å·¥å…·æ¨¡å—ï¼ˆé…ç½®ã€å®¢æˆ·ç«¯ã€åª’ä½“å¤„ç†ã€æ—¥å¿—ï¼‰
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ storage/              # ä¼šè¯ä¸æ—¥å¿—è¾“å‡º
â”œâ”€â”€ tests/                # æµ‹è¯•å¥—ä»¶
â”œâ”€â”€ setup.sh              # ä¸€é”®å®‰è£…ä¸åˆå§‹åŒ–è„šæœ¬
â”œâ”€â”€ pyproject.toml        # ä¾èµ–ç®¡ç†æºæ–‡ä»¶
â””â”€â”€ requirements.txt      # å¯¼å‡ºçš„ä¾èµ–æ¸…å•
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä¸€é”®åˆå§‹åŒ–ï¼ˆæ¨èï¼‰

```bash
bash setup.sh
```

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨åˆå§‹åŒ–

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt
pip install -e .

# åˆå§‹åŒ–é…ç½®æ–‡ä»¶
cp .env.example .env
cp config/profiles.example.yaml config/profiles.yaml
```

> [!NOTE]
> `config/profiles.yaml` ä¸­ `api_key_env` æŒ‡å‘çš„ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨ `.env` ä¸­å­˜åœ¨å¹¶å…·æœ‰æœ‰æ•ˆå€¼ã€‚
>
> ä¾‹å¦‚ï¼š`api_key_env: OPENAI_API_KEY` æ—¶ï¼Œ`.env` éœ€è¦æœ‰ `OPENAI_API_KEY = ...`ã€‚

> [!TIP]
> é¦–æ¬¡éªŒè¯é“¾è·¯ï¼Œæ¨èå…ˆä½¿ç”¨ CLI æµå¼æ¨¡å¼ï¼š
> ```bash
> llm-lab chat --stream
> ```

> [!TIP]
> å¦‚æœä½ ä½¿ç”¨ Conda ç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥è¿è¡Œï¼š
> ```bash
> conda run -n llm_test llm-lab chat
> ```

---

## âš™ï¸ é…ç½®è¯´æ˜

### 5.1 `.env` å¸¸ç”¨å­—æ®µ

```env
# æ ¸å¿ƒé…ç½®
LLM_LAB_PROFILE = dashscope_qwen      # é»˜è®¤ Profile
LLM_LAB_MODEL = qwen-max             # é»˜è®¤æ¨¡å‹
LLM_LAB_PROFILES_PATH = config/profiles.yaml  # é…ç½®æ–‡ä»¶è·¯å¾„

# API å¯†é’¥ï¼ˆæ ¹æ®éœ€è¦å¯ç”¨ï¼‰
OPENAI_API_KEY = sk-xxx              # OpenAI å¯†é’¥
DASHSCOPE_API_KEY = xxx             # é˜¿é‡Œé€šä¹‰åƒé—®å¯†é’¥
KIMI_API_KEY = xxx                   # Kimi å¯†é’¥
VOLCES_API_KEY = xxx                 # ç«å±±å¼•æ“å¯†é’¥
SILICON_API_KEY = xxx                # ç¡…åŸºæµåŠ¨å¯†é’¥
DEEPSEEK_API_KEY = xxx               # DeepSeek å¯†é’¥
GLM_API_KEY = xxx                    # æ™ºè°± AI å¯†é’¥
```

### 5.2 `config/profiles.yaml` ç¤ºä¾‹

```yaml
default_profile: dashscope_qwen

profiles:
  openai_default:
    base_url: https://api.openai.com/v1
    api_key_env: OPENAI_API_KEY
    default_model: gpt-4o-mini
    models:
      - gpt-4o-mini
      - gpt-4o

  dashscope_qwen:
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    api_key_env: DASHSCOPE_API_KEY
    default_model: qwen-max
    models:
      - qwen-max
      - qwen-plus
    enable_deep_thinking: false

  kimi_moonshot:
    base_url: https://api.moonshot.cn/v1
    api_key_env: KIMI_API_KEY
    default_model: moonshot-v1-8k
    models:
      - moonshot-v1-8k
      - moonshot-v1-32k

  volces_ark:
    base_url: https://ark.cn-beijing.volces.com/api/v3
    api_key_env: VOLCES_API_KEY
    default_model: Doubao-Seed-2.0-pro-260215
    models:
      - Doubao-Seed-2.0-pro-260215
      - Kimi-K2-Thinking-251104
      - DeepSeek-V3.2-251201
    model_aliases:
      Doubao-Seed-2.0-pro-260215: ep-your-endpoint-id
      Kimi-K2-Thinking-251104: ep-your-endpoint-id-2
      DeepSeek-V3.2-251201: ep-your-endpoint-id-3

  siliconflow_default:
    base_url: https://api.siliconflow.cn/v1
    api_key_env: SILICON_API_KEY
    default_model: Qwen/Qwen2.5-7B-Instruct
    models:
      - Qwen/Qwen2.5-7B-Instruct
      - deepseek-ai/DeepSeek-V3

  deepseek_default:
    base_url: https://api.deepseek.com/v1
    api_key_env: DEEPSEEK_API_KEY
    default_model: deepseek-chat
    models:
      - deepseek-chat
      - deepseek-reasoner

  glm_default:
    base_url: https://open.bigmodel.cn/api/paas/v4
    api_key_env: GLM_API_KEY
    default_model: glm-4-flash
    models:
      - glm-4-flash
      - glm-4-plus
```

> [!TIP]
> `models` ç”¨äºç»™æ¯ä¸ª Provider æä¾›å¯åˆ‡æ¢æ¨¡å‹åˆ—è¡¨ï¼ˆWeb/CLI åˆ‡æ¢æ—¶ä¼šè¯»å–è¿™ä¸ªåˆ—è¡¨ï¼‰ã€‚
> å¦‚æœä½ æœ‰æ›´å¤šå¯ç”¨æ¨¡å‹ï¼Œç›´æ¥ç»§ç»­è¿½åŠ åˆ°å¯¹åº” Provider çš„ `models`ã€‚
>
> `model_aliases` ç”¨äºâ€œæ˜¾ç¤ºå -> å®é™…è¯·æ±‚æ¨¡å‹ IDâ€æ˜ å°„ã€‚
> ä¾‹å¦‚ç•Œé¢æ˜¾ç¤º `Doubao-Seed-2.0-pro-260215`ï¼Œè¯·æ±‚æ—¶è‡ªåŠ¨ä½¿ç”¨ `ep-...` endpoint_idã€‚
>
> `enable_deep_thinking` ç”¨äºæ§åˆ¶æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ã€‚è‹¥é…ç½®ä¸º `true` ä½†å½“å‰æ¨¡å‹ä¸æ”¯æŒï¼Œ
> ç¨‹åºä¼šç»™å‡º warningï¼Œå¹¶è‡ªåŠ¨å›é€€åˆ°æ™®é€šæ¨¡å¼ç»§ç»­æ‰§è¡Œã€‚

### 5.3 é…ç½®ä¼˜å…ˆçº§

1. **å‘½ä»¤è¡Œå‚æ•°**ï¼ˆæœ€é«˜ï¼‰
2. **.env æ–‡ä»¶**ï¼ˆä¸­é—´ï¼‰
3. **profiles.yaml é»˜è®¤å€¼**ï¼ˆæœ€ä½ï¼‰

> [!NOTE]
> å½“ä½ æ˜¾å¼ä¼ å…¥ `--profile` ä½†æœªä¼  `--model` æ—¶ï¼Œä¼šä¼˜å…ˆä½¿ç”¨è¯¥ Profile åœ¨
> `profiles.yaml` ä¸­çš„ `default_model`ï¼Œé¿å…è¢« `.env` é‡Œçš„å…¨å±€ `LLM_LAB_MODEL` è¦†ç›–ã€‚

---

## ğŸ® å¯åŠ¨ç¤ºä¾‹

### CLI æ¨¡å¼

```bash
# åŸºç¡€èŠå¤©
llm-lab chat

# æµå¼è¾“å‡ºï¼ˆæ¨èï¼‰
llm-lab chat --stream

# ç‰¹å®š Profile å’Œæ¨¡å‹
llm-lab chat --profile dashscope_qwen --model qwen-max
llm-lab chat --profile kimi_moonshot --model moonshot-v1-8k
llm-lab chat --profile openai_default --model gpt-4o-mini
llm-lab chat --profile volces_ark --model ep-your-model-id
llm-lab chat --profile siliconflow_default --model Qwen/Qwen2.5-7B-Instruct
llm-lab chat --profile deepseek_default --model deepseek-chat
llm-lab chat --profile glm_default --model glm-4-flash

# ä¿å­˜ä¼šè¯
llm-lab chat --save-session
```

### WebUI æ¨¡å¼

```bash
# é»˜è®¤å¯åŠ¨ Streamlitï¼ˆé»˜è®¤ 8501 ç«¯å£ï¼‰
llm-lab web --host 127.0.0.1 --port 8501

# æ˜¾å¼æŒ‡å®š Streamlit
llm-lab web --ui streamlit --host 127.0.0.1 --port 8501

# Gradio ç•Œé¢ï¼ˆé»˜è®¤ 7860 ç«¯å£ï¼‰
llm-lab web --ui gradio --host 127.0.0.1 --port 7860

# FastAPI + HTML ç•Œé¢ï¼ˆé»˜è®¤ 8000 ç«¯å£ï¼‰
llm-lab web --ui fastapi --host 127.0.0.1 --port 8000
```

---

## ğŸ’¬ CLI å‘½ä»¤é€ŸæŸ¥

| å‘½ä»¤ | åŠŸèƒ½ | ç¤ºä¾‹ |
|---|---|---|
| `/help` | æŸ¥çœ‹æ‰€æœ‰å‘½ä»¤å¸®åŠ© | `/help` |
| `/status` | æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ | `/status` |
| `/profiles` | åˆ—å‡ºå¯ç”¨ Profile | `/profiles` |
| `/use <profile>` | åˆ‡æ¢ Profile | `/use dashscope_qwen` |
| `/model <model>` | åˆ‡æ¢æ¨¡å‹ | `/model qwen-max` |
| `/stream <on/off>` | è®¾ç½®æµå¼è¾“å‡º | `/stream on` |
| `/think <on/off>` | è®¾ç½® thinking æ¨¡å¼ | `/think on` |
| `/temp <float>` | è®¾ç½®æ¸©åº¦å‚æ•° | `/temp 0.7` |
| `/top_p <float>` | è®¾ç½® top-p å‚æ•° | `/top_p 0.9` |
| `/image <path>` | é™„åŠ å›¾ç‰‡ | `/image /path/to/image.jpg` |
| `/video <path>` | é™„åŠ è§†é¢‘ | `/video /path/to/video.mp4` |
| `/clear` | æ¸…ç©ºä¼šè¯ | `/clear` |
| `/save [name]` | ä¿å­˜ä¼šè¯ | `/save my_chat` |
| `/load <path>` | åŠ è½½ä¼šè¯ | `/load storage/conversations/my_chat.json` |
| `/exit` / `/quit` | é€€å‡ºç¨‹åº | `/exit` |

---

## ğŸ“¦ ä¾èµ–ç»´æŠ¤

æœ¬é¡¹ç›®é‡‡ç”¨åŒæ–‡ä»¶ä¾èµ–ç®¡ç†ï¼š

- **ä¸»ä¾èµ–æº**: `pyproject.toml`ï¼ˆä½¿ç”¨ Poetry ç®¡ç†ï¼‰
- **å¯¼å‡ºç»“æœ**: `requirements.txt`ï¼ˆç”¨äº pip å®‰è£…ï¼‰

åŒæ­¥å‘½ä»¤ï¼š

```bash
python scripts/sync_requirements.py
```

---

## â“ å¸¸è§é—®é¢˜

### æ•…éšœæ’æŸ¥

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|---|---|
| `llm-lab: command not found` | è¿è¡Œ `pip install -e .` é‡æ–°å®‰è£… |
| `Missing API key` | æ£€æŸ¥ `.env` æ–‡ä»¶æ˜¯å¦åŒ…å« `profiles.yaml` ä¸­å¼•ç”¨çš„ç¯å¢ƒå˜é‡ |
| è§†é¢‘å¤„ç†å¤±è´¥ | ç¡®è®¤å·²å®‰è£… `opencv-python-headless` åŒ… |
| ä¾èµ–å†²çª | å°è¯•ä½¿ç”¨ Conda ç¯å¢ƒæˆ–åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ |

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ **MIT License**ï¼Œè¯¦è§ `LICENSE` æ–‡ä»¶ã€‚

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

<div align="center">
  <p style="color: #666; font-size: 14px;">
    Built with â¤ï¸ by Brench
  </p>
</div>
